<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on marabunta</title>
    <link>https://marabunta.io/post/</link>
    <description>Recent content in Posts on marabunta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Nov 2018 15:13:13 +0100</lastBuildDate>
    
	<atom:link href="https://marabunta.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Problem to Solve</title>
      <link>https://marabunta.io/post/the-problem-to-solve/</link>
      <pubDate>Tue, 06 Nov 2018 15:13:13 +0100</pubDate>
      
      <guid>https://marabunta.io/post/the-problem-to-solve/</guid>
      <description>Cloud Backup Having to backup one, two or 10 systems is doable, but whey you need to backup hundreds and change frequently or temporally the credentials, bucket names etc things become complicated to manage.
One of the goals is to dynamically take advantage of the task payload in where the database credential could be passed, same as S3 access key etc, simplifying and securing with this the backup process at any scale.</description>
    </item>
    
    <item>
      <title>prototype</title>
      <link>https://marabunta.io/post/prototype/</link>
      <pubDate>Mon, 05 Nov 2018 11:02:09 +0100</pubDate>
      
      <guid>https://marabunta.io/post/prototype/</guid>
      <description>The big picture Tasks are stored in a MySQL, a worker (scheduler) periodically check the schedules and creates, updates or remove tasks from a live queue that uses redis, rabbitmq, nats, etc as a backend. One or more clients consume the queue and via a bidirectional gRPC stream dispatch the task to the proper client, if the task is finished, the client updates the result/status via gRPC Unary if not task is re-queued.</description>
    </item>
    
  </channel>
</rss>